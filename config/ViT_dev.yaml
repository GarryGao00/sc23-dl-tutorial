base: &base

  # Model config
  embed_dim: 384
  depth: 12
  dropout: 0
  patch_size: 8
  num_heads: 8

  # Training config
  img_size: [360, 720]
  dt: 1 
  global_batch_size: 16 # number of samples per training batch
  num_iters: 60000
  amp_mode: none
  enable_apex: False
  enable_jit: False
  expdir: '/logs'
  lr_schedule: 'cosine'
  lr: 5E-4
  warmup: 0
  optimizer: 'Adam'

  # Data
  data_loader_config: 'pytorch'
  num_data_workers: 0 # number of dataloader worker threads per proc
  n_in_channels: 20
  n_out_channels: 20
  train_data_path:   '/data/train'
  valid_data_path:   '/data/valid'
  inf_data_path:     '/data/test'
  time_means_path:   '/data/stats/time_means.npy'
  global_means_path: '/data/stats/global_means.npy'
  global_stds_path:  '/data/stats/global_stds.npy'

short_noopt:
  <<: *base
  num_data_workers: 0
  global_batch_size: 16

short: &short
  <<: *base
  num_data_workers: 8
  global_batch_size: 16

# Short config with full optimizations
short_opt: &opt
  <<: *short
  data_loader_config: 'dali'
  num_data_workers: 8
  num_iters: 30000

bs32_opt_nh4:
  <<: *opt
  global_batch_size: 32
  lr: 5e-4
  num_heads: 4

bs32_opt_nh2:
  <<: *opt
  global_batch_size: 32
  lr: 5e-4
  num_heads: 2

bs32_opt_e1024:
  <<: *opt
  global_batch_size: 32
  lr: 5e-4
  embed_dim: 1024

bs16_opt_p4:
  <<: *opt
  global_batch_size: 16
  lr: 5e-4
  patch_size: 4

bs32_opt:
  <<: *opt
  global_batch_size: 32
  lr: 5e-4

bs64_opt:
  <<: *opt
  global_batch_size: 64
  lr: 7.07e-4

bs128_opt:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3

bs256_opt:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3

bs512_opt:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3

bs1024_opt:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3

bs2048_opt:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3


## WARMUP 2.5k iters ------------------------------------------------

bs128_opt_w2p5:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  warmup: 2500

bs256_opt_w2p5:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  warmup: 2500

bs512_opt_w2p5:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  warmup: 2500

bs1024_opt_w2p5:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  warmup: 2500

bs2048_opt_w2p5:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  warmup: 2500




## WARMUP 1k iters, total iters 15k ------------------------------

bs128_opt_w1_ni15:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  warmup: 1000
  num_iters: 15000

bs256_opt_w1_ni15:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  warmup: 1000
  num_iters: 15000

bs512_opt_w1_ni15:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  warmup: 1000
  num_iters: 15000

bs1024_opt_w1_ni15:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  warmup: 1000
  num_iters: 15000

bs2048_opt_w1_ni15:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  warmup: 1000
  num_iters: 15000



## LAMB ---------------------------------------------------------

bs128_opt_lamb:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  optimizer: 'lamb'

bs256_opt_lamb:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  optimizer: 'lamb'

bs512_opt_lamb:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  optimizer: 'lamb'

bs1024_opt_lamb:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  optimizer: 'lamb'

bs2048_opt_lamb:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  optimizer: 'lamb'

#############

bs128_opt_w1_ni15_lamb:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'lamb'

bs256_opt_w1_ni15_lamb:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'lamb'

bs512_opt_w1_ni15_lamb:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'lamb'

bs1024_opt_w1_ni15_lamb:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'lamb'

bs2048_opt_w1_ni15_lamb:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'lamb'


## BETA_2 -------------------------------------------------------

bs128_opt_b95:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  optimizer: 'Adam_b2=0.95'

bs256_opt_b95:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  optimizer: 'Adam_b2=0.95'

bs512_opt_b95:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  optimizer: 'Adam_b2=0.95'

bs1024_opt_b95:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  optimizer: 'Adam_b2=0.95'

bs2048_opt_b95:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  optimizer: 'Adam_b2=0.95'


bs128_opt_b95_ni15:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 15000

bs256_opt_b95_ni12:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 12000

bs512_opt_b95_ni9:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 9000

bs1024_opt_b95_ni6:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 6000

bs2048_opt_b95_ni3:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 3000

bs128_opt_b95_ni15_2x:
  <<: *opt
  global_batch_size: 128
  lr: 2e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 15000

bs256_opt_b95_ni12_2x:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 12000

bs512_opt_b95_ni9_2x:
  <<: *opt
  global_batch_size: 512
  lr: 4e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 9000

bs1024_opt_b95_ni6_2x:
  <<: *opt
  global_batch_size: 1024
  lr: 5.66e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 6000

bs2048_opt_b95_ni3_2x:
  <<: *opt
  global_batch_size: 2048
  lr: 8e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 3000

bs512_opt_b95_2x_w500:
  <<: *opt
  global_batch_size: 512
  lr: 4e-3
  optimizer: 'Adam_b2=0.95'

bs1024_opt_b95_2x_w500:
  <<: *opt
  global_batch_size: 1024
  lr: 5.66e-3
  optimizer: 'Adam_b2=0.95'
  num_iters: 30000

bs2048_opt_b95_2x_w500:
  <<: *opt
  global_batch_size: 2048
  lr: 8e-3
  optimizer: 'Adam_b2=0.95'

bs32_opt_b95:
  <<: *opt
  global_batch_size: 32
  lr: 5e-4
  num_iters: 30000
  optimizer: 'Adam_b2=0.95'

bs64_opt_b95:
  <<: *opt
  global_batch_size: 64
  lr: 7.07e-4
  optimizer: 'Adam_b2=0.95'

bs16_opt_b95:
  <<: *opt
  global_batch_size: 16
  lr: 5e-4
  num_iters: 30000
  optimizer: 'Adam_b2=0.95'

###################################

bs128_opt_w1_ni15_b95:
  <<: *opt
  global_batch_size: 128
  lr: 1e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'Adam_b2=0.95'

bs256_opt_w1_ni15_b95:
  <<: *opt
  global_batch_size: 256
  lr: 1.41e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'Adam_b2=0.95'

bs512_opt_w1_ni15_b95:
  <<: *opt
  global_batch_size: 512
  lr: 2e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'Adam_b2=0.95'

bs1024_opt_w1_ni15_b95:
  <<: *opt
  global_batch_size: 1024
  lr: 2.83e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'Adam_b2=0.95'

bs2048_opt_w1_ni15_b95:
  <<: *opt
  global_batch_size: 2048
  lr: 4e-3
  warmup: 1000
  num_iters: 15000
  optimizer: 'Adam_b2=0.95'


